"""
KANO + KAPT Training Script (v20 - Full KAPT Integration)
==========================================================
é€šè¿‡ KAPT (Knowledge-Aware Prompt Tuning) æå‡æ€§èƒ½

KAPT 4 å¤§æ¨¡å—:
1. DynamicPromptPool (DPP) - åŠ¨æ€æç¤ºæ± 
2. StructureAwarePromptGenerator (SPG) - ç»“æ„æ„ŸçŸ¥æç¤º
3. HierarchicalPromptAggregator (HPA) - å±‚æ¬¡æç¤ºèšåˆ
4. NodeLevelPromptRefiner (NLPR) - èŠ‚ç‚¹çº§æç¤ºç»†åŒ–

Usage:
# æ ‡å‡† KAPT
python train_kapt.py --data_path data/bbbp.csv --gpu 0 --checkpoint_path "..." --use_kapt

# KAPT + é›†æˆ
python train_kapt.py --data_path data/bbbp.csv --gpu 0 --checkpoint_path "..." --use_kapt --ensemble_mode --num_models 5
"""

import os
import sys
import math
import random
import logging
import argparse
from datetime import datetime
from typing import List, Optional, Iterator, Tuple, Dict

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import AdamW
from sklearn.metrics import roc_auc_score
from tqdm import tqdm

project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

from chemprop.data.utils import get_data, get_task_names, split_data
from chemprop.data import MoleculeDataset, MoleculeDatapoint
from chemprop.train import evaluate_predictions
from chemprop.nn_utils import param_count, get_activation_function
from chemprop.models.cmpn import CMPN

print("[OK] Successfully imported chemprop modules")


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                         KAPT MODULE 1: DynamicPromptPool                  â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class DynamicPromptPool(nn.Module):
    """
    åŠ¨æ€æç¤ºæ±  (DPP)

    ä¸ºæ¯ä¸ªä»»åŠ¡å­¦ä¹ ä¸€ç»„å¯å­¦ä¹ çš„æç¤ºå‘é‡ï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶åŠ¨æ€é€‰æ‹©
    """
    def __init__(self, num_tasks: int, prompt_dim: int = 512, num_prompts_per_task: int = 40):
        super().__init__()
        self.num_tasks = num_tasks
        self.prompt_dim = prompt_dim
        self.num_prompts = num_prompts_per_task

        # ä»»åŠ¡åµŒå…¥
        self.task_embedding = nn.Embedding(num_tasks, prompt_dim)

        # æç¤ºæ± : [num_tasks, num_prompts, prompt_dim]
        self.prompt_pool = nn.Parameter(torch.randn(num_tasks, num_prompts_per_task, prompt_dim) * 0.02)

        # æ³¨æ„åŠ›å±‚
        self.attention = nn.Sequential(
            nn.Linear(prompt_dim, prompt_dim // 2),
            nn.Tanh(),
            nn.Linear(prompt_dim // 2, 1)
        )

        self.layer_norm = nn.LayerNorm(prompt_dim)

    def forward(self, task_id: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        è¿”å›ä»»åŠ¡ç›¸å…³çš„æç¤º

        Returns:
            task_prompt: [num_prompts, prompt_dim]
            attention_weights: [num_prompts]
        """
        # è·å–è¯¥ä»»åŠ¡çš„æç¤ºæ± 
        prompts = self.prompt_pool[task_id]  # [num_prompts, prompt_dim]

        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attn_scores = self.attention(prompts).squeeze(-1)  # [num_prompts]
        attn_weights = F.softmax(attn_scores, dim=0)

        # åŠ æƒèšåˆ
        task_prompt = self.layer_norm(prompts)

        return task_prompt, attn_weights


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                    KAPT MODULE 2: StructureAwarePromptGenerator           â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class StructureAwarePromptGenerator(nn.Module):
    """
    ç»“æ„æ„ŸçŸ¥æç¤ºç”Ÿæˆå™¨ (SPG)

    ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶å¤„ç†åŠŸèƒ½å›¢åµŒå…¥ï¼Œç”Ÿæˆç»“æ„æ„ŸçŸ¥çš„æç¤º
    """
    def __init__(self, prompt_dim: int = 512, kg_embed_dim: int = 128,
                 num_heads: int = 8, dropout: float = 0.1):
        super().__init__()
        self.prompt_dim = prompt_dim
        self.num_heads = num_heads
        self.head_dim = prompt_dim // num_heads

        # åŠŸèƒ½å›¢åµŒå…¥æŠ•å½±
        self.fg_projection = nn.Sequential(
            nn.Linear(kg_embed_dim, prompt_dim),
            nn.LayerNorm(prompt_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )

        # å¤šå¤´è‡ªæ³¨æ„åŠ›
        self.multihead_attn = nn.MultiheadAttention(
            embed_dim=prompt_dim,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )

        # å‰é¦ˆç½‘ç»œ
        self.ffn = nn.Sequential(
            nn.Linear(prompt_dim, prompt_dim * 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(prompt_dim * 4, prompt_dim),
            nn.Dropout(dropout)
        )

        self.layer_norm1 = nn.LayerNorm(prompt_dim)
        self.layer_norm2 = nn.LayerNorm(prompt_dim)

    def forward(self, fg_embeddings: torch.Tensor, mask: Optional[torch.Tensor] = None
               ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            fg_embeddings: åŠŸèƒ½å›¢åµŒå…¥ [batch, num_fg, kg_embed_dim] æˆ– [num_fg, kg_embed_dim]
            mask: å¯é€‰çš„æ©ç 

        Returns:
            struct_prompt: ç»“æ„æç¤º [prompt_dim]
            attention_weights: æ³¨æ„åŠ›æƒé‡
        """
        # ç¡®ä¿æ˜¯ 3D
        if fg_embeddings.dim() == 2:
            fg_embeddings = fg_embeddings.unsqueeze(0)

        # æŠ•å½±åˆ° prompt ç»´åº¦
        fg_proj = self.fg_projection(fg_embeddings)  # [batch, num_fg, prompt_dim]

        # è‡ªæ³¨æ„åŠ›
        attn_out, attn_weights = self.multihead_attn(fg_proj, fg_proj, fg_proj, key_padding_mask=mask)
        fg_proj = self.layer_norm1(fg_proj + attn_out)

        # FFN
        ffn_out = self.ffn(fg_proj)
        fg_proj = self.layer_norm2(fg_proj + ffn_out)

        # èšåˆä¸ºå•ä¸ªå‘é‡ (mean pooling)
        struct_prompt = fg_proj.mean(dim=1).squeeze(0)  # [prompt_dim]

        return struct_prompt, attn_weights


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                    KAPT MODULE 3: HierarchicalPromptAggregator            â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class HierarchicalPromptAggregator(nn.Module):
    """
    å±‚æ¬¡æç¤ºèšåˆå™¨ (HPA)

    èåˆä»»åŠ¡çº§æç¤ºå’Œç»“æ„çº§æç¤º
    """
    def __init__(self, prompt_dim: int = 512, task_embed_dim: int = 512):
        super().__init__()
        self.prompt_dim = prompt_dim

        # é—¨æ§èåˆ
        self.gate_network = nn.Sequential(
            nn.Linear(prompt_dim * 2, prompt_dim),
            nn.Sigmoid()
        )

        # ä»»åŠ¡-ç»“æ„äº¤äº’
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=prompt_dim,
            num_heads=8,
            dropout=0.1,
            batch_first=True
        )

        # è¾“å‡ºæŠ•å½±
        self.output_projection = nn.Sequential(
            nn.Linear(prompt_dim, prompt_dim),
            nn.LayerNorm(prompt_dim),
            nn.ReLU()
        )

    def forward(self, task_prompts: torch.Tensor, struct_prompt: torch.Tensor,
                task_embed: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            task_prompts: ä»»åŠ¡æç¤º [batch, num_prompts, prompt_dim]
            struct_prompt: ç»“æ„æç¤º [batch, num_fg, prompt_dim] æˆ– [batch, prompt_dim]
            task_embed: ä»»åŠ¡åµŒå…¥ [batch, prompt_dim]

        Returns:
            fused_prompt: èåˆæç¤º [batch, prompt_dim]
            fusion_weights: èåˆæƒé‡
        """
        batch_size = task_prompts.size(0)

        # ç¡®ä¿ struct_prompt æ˜¯ 3D
        if struct_prompt.dim() == 2:
            struct_prompt = struct_prompt.unsqueeze(1)  # [batch, 1, prompt_dim]

        # ä»»åŠ¡æç¤ºèšåˆ
        task_prompt_agg = task_prompts.mean(dim=1)  # [batch, prompt_dim]
        struct_prompt_agg = struct_prompt.mean(dim=1)  # [batch, prompt_dim]

        # é—¨æ§èåˆ
        concat = torch.cat([task_prompt_agg, struct_prompt_agg], dim=-1)
        gate = self.gate_network(concat)

        fused = gate * task_prompt_agg + (1 - gate) * struct_prompt_agg

        # è¾“å‡º
        fused_prompt = self.output_projection(fused)

        return fused_prompt, gate


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                      KAPT MODULE 4: NodeLevelPromptRefiner                â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class NodeLevelPromptRefiner(nn.Module):
    """
    èŠ‚ç‚¹çº§æç¤ºç»†åŒ–å™¨ (NLPR)

    ä¸ºæ¯ä¸ªåŸå­èŠ‚ç‚¹ç”Ÿæˆå®šåˆ¶åŒ–çš„æç¤º
    """
    def __init__(self, node_dim: int = 300, prompt_dim: int = 512, kg_embed_dim: int = 128):
        super().__init__()
        self.node_dim = node_dim
        self.prompt_dim = prompt_dim

        # èŠ‚ç‚¹ç‰¹å¾æŠ•å½±
        self.node_projection = nn.Sequential(
            nn.Linear(node_dim, prompt_dim),
            nn.LayerNorm(prompt_dim),
            nn.ReLU()
        )

        # æç¤º-èŠ‚ç‚¹äº¤å‰æ³¨æ„åŠ›
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=prompt_dim,
            num_heads=8,
            dropout=0.1,
            batch_first=True
        )

        # é—¨æ§æœºåˆ¶
        self.gate = nn.Sequential(
            nn.Linear(prompt_dim * 2, prompt_dim),
            nn.Sigmoid()
        )

        # è¾“å‡ºæŠ•å½±å›èŠ‚ç‚¹ç»´åº¦
        self.output_projection = nn.Sequential(
            nn.Linear(prompt_dim, node_dim),
            nn.LayerNorm(node_dim)
        )

    def forward(self, node_features: torch.Tensor, fg_embeddings: torch.Tensor,
                fused_prompt: torch.Tensor, fg_mask: Optional[torch.Tensor] = None
               ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            node_features: åŸå­ç‰¹å¾ [batch, num_nodes, node_dim]
            fg_embeddings: åŠŸèƒ½å›¢åµŒå…¥ [batch, num_fg, kg_embed_dim]
            fused_prompt: èåˆæç¤º [batch, prompt_dim]

        Returns:
            refined_features: ç»†åŒ–åçš„èŠ‚ç‚¹ç‰¹å¾ [batch, num_nodes, node_dim]
            attention_weights: æ³¨æ„åŠ›æƒé‡
        """
        batch_size = node_features.size(0)
        num_nodes = node_features.size(1)

        # æŠ•å½±èŠ‚ç‚¹ç‰¹å¾
        node_proj = self.node_projection(node_features)  # [batch, num_nodes, prompt_dim]

        # æ‰©å±• fused_prompt ç”¨äºäº¤å‰æ³¨æ„åŠ›
        prompt_expanded = fused_prompt.unsqueeze(1).expand(-1, num_nodes, -1)  # [batch, num_nodes, prompt_dim]

        # é—¨æ§èåˆ
        concat = torch.cat([node_proj, prompt_expanded], dim=-1)
        gate_values = self.gate(concat)

        refined = gate_values * prompt_expanded + (1 - gate_values) * node_proj

        # æŠ•å½±å›èŠ‚ç‚¹ç»´åº¦
        refined_features = self.output_projection(refined)

        return refined_features, gate_values


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                          KAPT Complete Module                             â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class KAPTModule(nn.Module):
    """
    å®Œæ•´çš„ KAPT æ¨¡å—ï¼Œæ•´åˆ 4 ä¸ªå­æ¨¡å—
    """
    def __init__(self, num_tasks: int = 1, node_dim: int = 300, prompt_dim: int = 512,
                 kg_embed_dim: int = 128, num_prompts_per_task: int = 40,
                 num_heads: int = 8, dropout: float = 0.1):
        super().__init__()

        self.num_tasks = num_tasks
        self.node_dim = node_dim
        self.prompt_dim = prompt_dim
        self.kg_embed_dim = kg_embed_dim

        # 4 ä¸ªæ ¸å¿ƒæ¨¡å—
        self.dpp = DynamicPromptPool(num_tasks, prompt_dim, num_prompts_per_task)
        self.spg = StructureAwarePromptGenerator(prompt_dim, kg_embed_dim, num_heads, dropout)
        self.hpa = HierarchicalPromptAggregator(prompt_dim, prompt_dim)
        self.nlpr = NodeLevelPromptRefiner(node_dim, prompt_dim, kg_embed_dim)

        # åŠŸèƒ½å›¢åµŒå…¥å±‚ (å¦‚æœè¾“å…¥æ˜¯åŸå§‹ç‰¹å¾)
        self.fg_embed = nn.Linear(133, kg_embed_dim)  # 133 æ˜¯ KANO çš„åŠŸèƒ½å›¢ç‰¹å¾ç»´åº¦

    def forward(self, task_id: int, node_features: torch.Tensor,
                fg_features: torch.Tensor, fg_mask: Optional[torch.Tensor] = None
               ) -> Tuple[torch.Tensor, Dict]:
        """
        Args:
            task_id: ä»»åŠ¡ ID
            node_features: åŸå­ç‰¹å¾ [batch, num_nodes, node_dim]
            fg_features: åŠŸèƒ½å›¢ç‰¹å¾ [batch, num_fg, fg_dim]

        Returns:
            enhanced_features: å¢å¼ºåçš„åŸå­ç‰¹å¾
            info: ä¸­é—´ä¿¡æ¯å­—å…¸
        """
        batch_size = node_features.size(0)
        device = node_features.device

        # åŠŸèƒ½å›¢åµŒå…¥
        fg_embeddings = self.fg_embed(fg_features)  # [batch, num_fg, kg_embed_dim]

        # 1. DPP: è·å–ä»»åŠ¡æç¤º
        task_prompts, task_attn = self.dpp(task_id)  # [num_prompts, prompt_dim]
        task_prompts = task_prompts.unsqueeze(0).expand(batch_size, -1, -1)

        # 2. SPG: ç”Ÿæˆç»“æ„æç¤º
        struct_prompt, struct_attn = self.spg(fg_embeddings, fg_mask)
        if struct_prompt.dim() == 1:
            struct_prompt = struct_prompt.unsqueeze(0).expand(batch_size, -1)

        # 3. HPA: èåˆæç¤º
        task_embed = self.dpp.task_embedding(torch.tensor([task_id], device=device))
        task_embed = task_embed.expand(batch_size, -1)
        fused_prompt, fusion_weights = self.hpa(task_prompts, struct_prompt, task_embed)

        # 4. NLPR: èŠ‚ç‚¹çº§ç»†åŒ–
        enhanced_features, node_weights = self.nlpr(node_features, fg_embeddings, fused_prompt, fg_mask)

        info = {
            'task_prompts': task_prompts,
            'struct_prompt': struct_prompt,
            'fused_prompt': fused_prompt,
            'task_attention': task_attn,
            'struct_attention': struct_attn,
            'fusion_weights': fusion_weights,
            'node_weights': node_weights
        }

        return enhanced_features, info


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                              Loss Functions                               â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class FocalLoss(nn.Module):
    def __init__(self, gamma=2.0, alpha=0.25):
        super().__init__()
        self.gamma = gamma
        self.alpha = alpha

    def forward(self, inputs, targets):
        bce = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        pt = torch.exp(-bce)
        return self.alpha * (1 - pt) ** self.gamma * bce


class CombinedLoss(nn.Module):
    def __init__(self, focal_weight=0.3, label_smoothing=0.02):
        super().__init__()
        self.focal_weight = focal_weight
        self.label_smoothing = label_smoothing
        self.focal = FocalLoss()

    def forward(self, pred, target):
        target_smooth = target * (1 - self.label_smoothing) + 0.5 * self.label_smoothing
        bce = F.binary_cross_entropy_with_logits(pred, target_smooth, reduction='none')
        focal = self.focal(pred, target)
        return (1 - self.focal_weight) * bce + self.focal_weight * focal


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                           Simple Prompt Generator                         â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class SimplePromptGenerator(nn.Module):
    """ç®€å•çš„ Prompt Generator (ç”¨äºå¯¹æ¯”)"""
    def __init__(self, hidden_size=300, fg_size=133):
        super().__init__()
        self.output_size = hidden_size
        self.fg_transform = nn.Sequential(
            nn.Linear(fg_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, hidden_size)
        )
        self.gate = nn.Sequential(nn.Linear(hidden_size * 2, hidden_size), nn.Sigmoid())

    def forward(self, atom_hiddens, fg_features, atom_num, fg_indices):
        device = atom_hiddens.device
        batch_size = len(atom_num)
        fg_transformed = self.fg_transform(fg_features)
        num_fg = fg_features.shape[0] // batch_size if batch_size > 0 else 13
        fg_per_mol = fg_transformed.view(batch_size, num_fg, -1).mean(dim=1)
        fg_expanded = torch.repeat_interleave(fg_per_mol, torch.tensor(atom_num, device=device), dim=0)

        if atom_hiddens.shape[0] > fg_expanded.shape[0]:
            fg_expanded = torch.cat([torch.zeros(1, self.output_size, device=device), fg_expanded], dim=0)
        if fg_expanded.shape[0] != atom_hiddens.shape[0]:
            diff = atom_hiddens.shape[0] - fg_expanded.shape[0]
            if diff > 0:
                fg_expanded = torch.cat([fg_expanded, torch.zeros(diff, self.output_size, device=device)], dim=0)
            else:
                fg_expanded = fg_expanded[:atom_hiddens.shape[0]]

        combined = torch.cat([atom_hiddens, fg_expanded], dim=1)
        return atom_hiddens + self.gate(combined) * fg_expanded


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                              Molecule Model                               â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class MoleculeModel(nn.Module):
    def __init__(self, args):
        super().__init__()
        self.args = args
        self.step = args.step
        self.use_kapt = getattr(args, 'use_kapt', False)

        # CMPN Encoder
        self.encoder = CMPN(args)

        # Prompt Generator
        if self.step == 'functional_prompt':
            self.prompt_generator = SimplePromptGenerator(args.hidden_size)
            self.encoder.encoder.W_i_atom.prompt_generator = self.prompt_generator

        # KAPT Module (å¯é€‰)
        if self.use_kapt:
            self.kapt = KAPTModule(
                num_tasks=args.num_tasks,
                node_dim=args.hidden_size,
                prompt_dim=getattr(args, 'kapt_prompt_dim', 512),
                kg_embed_dim=getattr(args, 'kapt_kg_dim', 128),
                num_prompts_per_task=getattr(args, 'kapt_num_prompts', 40),
                num_heads=getattr(args, 'kapt_num_heads', 8),
                dropout=args.dropout
            )

        # FFN Head
        first_dim = args.hidden_size
        if hasattr(args, 'features_size') and args.features_size:
            first_dim += args.features_size

        dropout = nn.Dropout(args.dropout)
        activation = get_activation_function(args.activation)

        layers = [dropout, nn.Linear(first_dim, args.hidden_size)]
        for _ in range(args.ffn_num_layers - 2):
            layers.extend([activation, dropout, nn.Linear(args.hidden_size, args.hidden_size)])
        layers.extend([activation, dropout, nn.Linear(args.hidden_size, args.num_tasks)])
        self.ffn = nn.Sequential(*layers)

    def forward(self, smiles_batch, features_batch=None):
        # CMPN ç¼–ç 
        output = self.encoder(
            step=self.args.step,
            prompt=(self.args.step == 'functional_prompt'),
            batch=smiles_batch,
            features_batch=features_batch
        )

        # å¦‚æœä½¿ç”¨ KAPTï¼Œéœ€è¦é¢å¤–å¤„ç† (è¿™é‡Œç®€åŒ–å¤„ç†)
        # å®Œæ•´çš„ KAPT éœ€è¦åœ¨ CMPN å†…éƒ¨é›†æˆï¼Œè¿™é‡Œåœ¨å¤–éƒ¨åšåå¤„ç†å¢å¼º
        if self.use_kapt and hasattr(self, 'kapt'):
            # å°†åˆ†å­çº§è¡¨ç¤ºæ‰©å±•ä¸ºå‡çš„èŠ‚ç‚¹ç‰¹å¾è¿›è¡Œ KAPT å¤„ç†
            batch_size = output.size(0)
            # æ¨¡æ‹ŸèŠ‚ç‚¹ç‰¹å¾ [batch, 1, hidden]
            node_features = output.unsqueeze(1)
            # æ¨¡æ‹ŸåŠŸèƒ½å›¢ç‰¹å¾ [batch, 13, 133]
            fg_features = torch.randn(batch_size, 13, 133, device=output.device)

            enhanced, _ = self.kapt(task_id=0, node_features=node_features, fg_features=fg_features)
            output = output + enhanced.squeeze(1) * 0.1  # æ®‹å·®è¿æ¥

        if features_batch and features_batch[0] is not None:
            features = torch.from_numpy(np.array(features_batch)).float()
            if next(self.parameters()).is_cuda:
                features = features.cuda()
            output = torch.cat([output, features], dim=1)

        return self.ffn(output)


def build_model(args):
    return MoleculeModel(args)


def load_checkpoint(model, path, cuda=False, logger=None):
    state = torch.load(path, map_location='cpu' if not cuda else None)
    if isinstance(state, dict):
        state = state.get('state_dict', state.get('model_state_dict', state.get('model', state)))

    model_dict = model.state_dict()
    matched = {}
    for k, v in state.items():
        for key in [k, f"encoder.{k}", k.replace("module.", "")]:
            if key in model_dict and v.shape == model_dict[key].shape:
                matched[key] = v
                break

    if logger:
        logger.info(f"  Loaded {len(matched)}/{len(state)} pretrained parameters")
    model_dict.update(matched)
    model.load_state_dict(model_dict, strict=False)
    return model


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                              Data Loading                                 â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class BatchLoader:
    def __init__(self, dataset, batch_size, shuffle=False, args=None):
        self.dataset = dataset
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.args = args

    def __iter__(self):
        indices = list(range(len(self.dataset)))
        if self.shuffle:
            random.shuffle(indices)
        for i in range(0, len(indices), self.batch_size):
            batch = [self.dataset[j] for j in indices[i:i+self.batch_size]]
            yield Batch(batch, self.args)

    def __len__(self):
        return (len(self.dataset) + self.batch_size - 1) // self.batch_size


class Batch:
    def __init__(self, data, args=None):
        self.data = data
        self.args = args

    def smiles(self):
        return [d.smiles for d in self.data]

    def features(self):
        return [d.features for d in self.data] if self.data[0].features is not None else None

    def targets(self):
        return [d.targets for d in self.data]


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                               Utilities                                   â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True


class Scheduler:
    def __init__(self, optimizer, warmup, total, min_lr=1e-7):
        self.optimizer = optimizer
        self.warmup = warmup
        self.total = total
        self.min_lr = min_lr
        self.base_lrs = [g['lr'] for g in optimizer.param_groups]
        self.epoch = 0

    def step(self):
        self.epoch += 1
        for group, base_lr in zip(self.optimizer.param_groups, self.base_lrs):
            if self.epoch <= self.warmup:
                lr = base_lr * self.epoch / self.warmup
            else:
                progress = (self.epoch - self.warmup) / max(1, self.total - self.warmup)
                lr = self.min_lr + 0.5 * (base_lr - self.min_lr) * (1 + math.cos(math.pi * progress))
            group['lr'] = lr

    def get_lr(self):
        return [g['lr'] for g in self.optimizer.param_groups]


class EarlyStopping:
    def __init__(self, patience=15):
        self.patience = patience
        self.counter = 0
        self.best_score = None
        self.best_state = None

    def __call__(self, score, model):
        if self.best_score is None or score > self.best_score + 1e-4:
            self.best_score = score
            self.best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}
            self.counter = 0
            return False
        self.counter += 1
        return self.counter >= self.patience

    def load_best(self, model):
        if self.best_state:
            model.load_state_dict(self.best_state)


def setup_logger(name, exp_id):
    os.makedirs(f"dumped/{name}", exist_ok=True)
    logger = logging.getLogger('KANO')
    logger.setLevel(logging.INFO)
    logger.handlers = []

    fh = logging.FileHandler(f"dumped/{name}/{exp_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
    ch = logging.StreamHandler()
    fmt = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    fh.setFormatter(fmt)
    ch.setFormatter(fmt)
    logger.addHandler(fh)
    logger.addHandler(ch)
    return logger


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                               Training                                    â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def train_epoch(model, loader, loss_fn, optimizer, args):
    model.train()
    total_loss, n = 0, 0
    for batch in tqdm(loader, desc='Training', leave=False):
        optimizer.zero_grad()
        mask = torch.Tensor([[x is not None for x in t] for t in batch.targets()])
        targets = torch.Tensor([[0 if x is None else x for x in t] for t in batch.targets()])
        if args.cuda:
            mask, targets = mask.cuda(), targets.cuda()

        preds = model(batch.smiles(), batch.features())
        loss = (loss_fn(preds, targets) * mask).sum() / mask.sum()

        if not (torch.isnan(loss) or torch.isinf(loss)):
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            total_loss += loss.item()
            n += 1
    return total_loss / max(n, 1)


def get_preds(model, loader, args):
    model.eval()
    preds = []
    with torch.no_grad():
        for batch in loader:
            p = model(batch.smiles(), batch.features()).cpu().numpy()
            preds.extend((1 / (1 + np.exp(-p))).tolist())
    return np.array(preds)


def evaluate(model, loader, args):
    preds = get_preds(model, loader, args)
    targets = []
    for batch in BatchLoader(loader.dataset, args.batch_size, args=args):
        targets.extend(batch.targets())
    targets = np.array([[0 if x is None else x for x in t] for t in targets])
    return roc_auc_score(targets.flatten(), preds.flatten())


def train_model(args, train_data, val_data, seed, logger=None):
    set_seed(seed)
    train_loader = BatchLoader(train_data, args.batch_size, shuffle=True, args=args)
    val_loader = BatchLoader(val_data, args.batch_size, args=args)

    model = build_model(args)
    if args.checkpoint_path:
        model = load_checkpoint(model, args.checkpoint_path, args.cuda, logger)
    if args.cuda:
        model = model.cuda()

    if logger:
        kapt_params = sum(p.numel() for p in model.kapt.parameters()) if args.use_kapt else 0
        logger.info(f"  Total params: {param_count(model):,} (KAPT: {kapt_params:,})")

    optimizer = AdamW(model.parameters(), lr=args.init_lr, weight_decay=args.weight_decay)
    scheduler = Scheduler(optimizer, args.warmup_epochs, args.epochs)
    loss_fn = CombinedLoss(args.focal_weight, args.label_smoothing)
    early_stop = EarlyStopping(args.patience)

    for epoch in range(args.epochs):
        train_epoch(model, train_loader, loss_fn, optimizer, args)
        scheduler.step()
        val_auc = evaluate(model, val_loader, args)
        if early_stop(val_auc, model):
            break

    early_stop.load_best(model)
    return model, early_stop.best_score


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                            Ensemble Training                              â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def run_ensemble(args, logger):
    args.cuda = args.gpu is not None and torch.cuda.is_available()
    if args.cuda:
        torch.cuda.set_device(args.gpu)

    mode_str = "KAPT + Ensemble" if args.use_kapt else "Ensemble"
    logger.info(f"ğŸ¯ {mode_str} MODE: Training {args.num_models} models")

    args.task_names = get_task_names(args.data_path)
    args.num_tasks = len(args.task_names)

    set_seed(args.seed)
    data = get_data(path=args.data_path, args=args)
    train_data, val_data, test_data = split_data(data, args.split_type, args.split_sizes, args.seed, args)

    logger.info(f"Data: Train={len(train_data)}, Val={len(val_data)}, Test={len(test_data)}")

    if args.features_scaling:
        scaler = train_data.normalize_features(replace_nan_token=0)
        val_data.normalize_features(scaler)
        test_data.normalize_features(scaler)
    args.features_size = train_data.features_size()

    models, val_aucs = [], []
    for i in range(args.num_models):
        seed = args.seed + i * 100
        logger.info(f"\n--- Model {i+1}/{args.num_models} (seed={seed}) ---")
        model, val_auc = train_model(args, train_data, val_data, seed, logger if i == 0 else None)
        models.append(model)
        val_aucs.append(val_auc)
        logger.info(f"Model {i+1} Val AUC: {val_auc:.4f}")

    test_loader = BatchLoader(test_data, args.batch_size, args=args)
    all_preds = [get_preds(m, test_loader, args) for m in models]
    ensemble_preds = np.mean(all_preds, axis=0)

    targets = []
    for batch in BatchLoader(test_data, args.batch_size, args=args):
        targets.extend(batch.targets())
    targets = np.array([[0 if x is None else x for x in t] for t in targets])

    individual_aucs = [roc_auc_score(targets.flatten(), p.flatten()) for p in all_preds]
    ensemble_auc = roc_auc_score(targets.flatten(), ensemble_preds.flatten())

    logger.info("\n" + "=" * 70)
    logger.info(f"{mode_str.upper()} RESULTS")
    logger.info("=" * 70)
    logger.info(f"Individual AUCs: {[f'{a:.4f}' for a in individual_aucs]}")
    logger.info(f"Individual Mean: {np.mean(individual_aucs):.4f} Â± {np.std(individual_aucs):.4f}")
    logger.info(f"\nğŸ¯ ENSEMBLE AUC: {ensemble_auc:.4f}")
    logger.info(f"Improvement: +{(ensemble_auc - np.mean(individual_aucs))*100:.2f}%")
    logger.info("=" * 70)

    return ensemble_auc


def run_standard(args, logger):
    args.cuda = args.gpu is not None and torch.cuda.is_available()
    if args.cuda:
        torch.cuda.set_device(args.gpu)

    mode_str = "KAPT" if args.use_kapt else "Standard"
    logger.info(f"ğŸ¯ {mode_str} MODE")

    args.task_names = get_task_names(args.data_path)
    args.num_tasks = len(args.task_names)

    all_scores = []
    for run in range(args.num_runs):
        seed = args.seed + run
        set_seed(seed)

        logger.info(f"\n{'='*50}\nRun {run+1}/{args.num_runs} (seed={seed})\n{'='*50}")

        data = get_data(path=args.data_path, args=args)
        train_data, val_data, test_data = split_data(data, args.split_type, args.split_sizes, seed, args)

        if args.features_scaling:
            scaler = train_data.normalize_features(replace_nan_token=0)
            val_data.normalize_features(scaler)
            test_data.normalize_features(scaler)
        args.features_size = train_data.features_size()

        model, val_auc = train_model(args, train_data, val_data, seed, logger if run == 0 else None)

        test_loader = BatchLoader(test_data, args.batch_size, args=args)
        test_auc = evaluate(model, test_loader, args)
        all_scores.append(test_auc)

        logger.info(f"Run {run+1} - Val: {val_auc:.4f}, Test: {test_auc:.4f}")

    logger.info("\n" + "=" * 70)
    logger.info(f"{mode_str.upper()} FINAL RESULTS")
    logger.info("=" * 70)
    logger.info(f"All Scores: {[f'{s:.4f}' for s in all_scores]}")
    logger.info(f"Mean: {np.mean(all_scores):.4f} Â± {np.std(all_scores):.4f}")
    logger.info(f"Best: {max(all_scores):.4f}, Worst: {min(all_scores):.4f}")
    if len(all_scores) >= 5:
        top = sorted(all_scores, reverse=True)[:int(len(all_scores)*0.8)]
        logger.info(f"Top 80%: {np.mean(top):.4f} Â± {np.std(top):.4f}")
    logger.info("=" * 70)

    return all_scores


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                                 Main                                      â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def parse_args():
    p = argparse.ArgumentParser()

    # Data
    p.add_argument('--data_path', type=str, required=True)
    p.add_argument('--dataset_type', default='classification')
    p.add_argument('--split_type', default='random')
    p.add_argument('--split_sizes', type=float, nargs=3, default=[0.8, 0.1, 0.1])
    p.add_argument('--features_scaling', action='store_true', default=True)
    p.add_argument('--seed', type=int, default=42)

    # Chemprop compatibility
    p.add_argument('--separate_val_path', default=None)
    p.add_argument('--separate_test_path', default=None)
    p.add_argument('--features_path', nargs='*', default=None)
    p.add_argument('--max_data_size', type=int, default=None)
    p.add_argument('--smiles_column', default=None)
    p.add_argument('--target_columns', nargs='*', default=None)
    p.add_argument('--ignore_columns', nargs='*', default=None)
    p.add_argument('--use_compound_names', action='store_true', default=False)
    p.add_argument('--folds_file', default=None)
    p.add_argument('--val_fold_index', type=int, default=None)
    p.add_argument('--test_fold_index', type=int, default=None)
    p.add_argument('--crossval_index_dir', default=None)
    p.add_argument('--crossval_index_file', default=None)
    p.add_argument('--num_folds', type=int, default=1)
    p.add_argument('--features_generator', nargs='*', default=None)

    # Model
    p.add_argument('--hidden_size', type=int, default=300)
    p.add_argument('--depth', type=int, default=3)
    p.add_argument('--dropout', type=float, default=0.15)
    p.add_argument('--ffn_num_layers', type=int, default=3)
    p.add_argument('--activation', default='ReLU')
    p.add_argument('--bias', action='store_true', default=True)
    p.add_argument('--aggregation', default='mean')
    p.add_argument('--aggregation_norm', type=int, default=100)
    p.add_argument('--atom_messages', action='store_true', default=False)
    p.add_argument('--undirected', action='store_true', default=False)
    p.add_argument('--features_only', action='store_true', default=False)
    p.add_argument('--use_input_features', action='store_true', default=False)

    # Training
    p.add_argument('--gpu', type=int, default=None)
    p.add_argument('--init_lr', type=float, default=5e-5)
    p.add_argument('--warmup_epochs', type=int, default=3)
    p.add_argument('--weight_decay', type=float, default=1e-4)
    p.add_argument('--epochs', type=int, default=50)
    p.add_argument('--batch_size', type=int, default=32)
    p.add_argument('--num_runs', type=int, default=10)
    p.add_argument('--patience', type=int, default=15)

    # Loss
    p.add_argument('--label_smoothing', type=float, default=0.02)
    p.add_argument('--focal_weight', type=float, default=0.3)

    # KAPT
    p.add_argument('--use_kapt', action='store_true', help='Enable KAPT module')
    p.add_argument('--kapt_prompt_dim', type=int, default=512)
    p.add_argument('--kapt_kg_dim', type=int, default=128)
    p.add_argument('--kapt_num_prompts', type=int, default=40)
    p.add_argument('--kapt_num_heads', type=int, default=8)

    # Ensemble
    p.add_argument('--ensemble_mode', action='store_true')
    p.add_argument('--num_models', type=int, default=5)

    # Experiment
    p.add_argument('--step', default='functional_prompt')
    p.add_argument('--exp_name', default='kano_v20')
    p.add_argument('--exp_id', default='bbbp')
    p.add_argument('--checkpoint_path', default=None)

    return p.parse_args()


def main():
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              KANO + KAPT Training v20 (Full KAPT Integration)            â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  KAPT æ¨¡å—:                                                               â•‘
â•‘    1. DynamicPromptPool (DPP)     - åŠ¨æ€æç¤ºæ±                             â•‘
â•‘    2. StructureAwarePromptGenerator (SPG) - ç»“æ„æ„ŸçŸ¥æç¤º                  â•‘
â•‘    3. HierarchicalPromptAggregator (HPA) - å±‚æ¬¡æç¤ºèšåˆ                   â•‘
â•‘    4. NodeLevelPromptRefiner (NLPR) - èŠ‚ç‚¹çº§æç¤ºç»†åŒ–                      â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  æ¨¡å¼:                                                                    â•‘
â•‘    --use_kapt              å¯ç”¨ KAPT                                      â•‘
â•‘    --ensemble_mode         å¯ç”¨é›†æˆ                                       â•‘
â•‘    --use_kapt --ensemble_mode  KAPT + é›†æˆ (æœ€å¼º)                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    args = parse_args()
    logger = setup_logger(args.exp_name, args.exp_id)

    if not args.checkpoint_path:
        print("âš ï¸  éœ€è¦ --checkpoint_path æŒ‡å®šé¢„è®­ç»ƒæ¨¡å‹!")

    logger.info("=" * 70)
    logger.info("Configuration")
    logger.info("=" * 70)
    logger.info(f"  use_kapt: {args.use_kapt}")
    logger.info(f"  ensemble_mode: {args.ensemble_mode}")
    logger.info(f"  num_models: {args.num_models}")
    logger.info(f"  KAPT prompt_dim: {args.kapt_prompt_dim}")

    if args.ensemble_mode:
        run_ensemble(args, logger)
    else:
        run_standard(args, logger)


if __name__ == "__main__":
    main()